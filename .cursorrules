# Cursor Rules

## üîÑ Session Continuity

**‚ö†Ô∏è ALWAYS run at conversation start (no exceptions):**
1. Run `git --no-pager log --stat -5`
2. Run `python cursor-scripts/cursor_usage.py reminder --once`
3. Run `docker ps` for environment detection
4. Run `python cursor-scripts/startup_cards.py` for daily digest + quiz
5. Summarize recent work + environment status (CLI/IDE, Docker running/not)

**Then:**
- User gave specific task ‚Üí Do the task immediately after summary
- User says "resume", "continue" ‚Üí Resume previous work
- Otherwise ‚Üí Ask: "Resume from where we left off, or start something new?"

**No skipping:** These checks surface critical context (Docker status, recent changes) that affects task execution.

---

### üîÑ New Chat

**Trigger:** User says "new chat", "start fresh", "clear chat", etc.

**Run:**
```bash
./cursor-scripts/cursor-new-chat.sh
```

This exports the current chat and clears history. User then restarts CLI agent.

---

### üìù Conversation Export

**Trigger (immediate, no confirmation):**
- `/e` - Export conversation

**Run:**
```bash
./cursor-scripts/export-chat.sh
```

**Confirm:** "Exported to `cursor-chats/CURSOR-CLI_YYYY-MM-DD.md`"

---

### üö´ Workspace Boundaries

**DO NOT access files outside workspace without explicit permission.**

Includes: `~/.zshrc`, `~/.config/*`, `~/.ssh/*`, any external path

**Workflow for external files:**
1. Ask: "I need to edit `~/.zshrc`. Can you copy it to `./temp/`?"
2. User copies to `./temp/`
3. AI edits `./temp/<filename>`
4. User copies back, confirms "done"
5. AI cleans up: `rm -rf ./temp/* ./temp/.* 2>/dev/null; true`

**Exception:** Only if user explicitly instructs (e.g., "read my ~/.zshrc")

---

### üîç Environment Detection

Determined by `docker ps` in startup checks.

| Mode | Meaning |
|------|---------|
| **CLI** | Docker running ‚Äî full shell access, run commands directly |
| **IDE** | Docker not running ‚Äî provide commands for user to run manually |

| Feature | CLI | IDE |
|---------|-----|-----|
| Docker Access | ‚úÖ | ‚ùå |
| Web Search | `web_search.py` | `WebFetch` |
| Browser MCP | ‚úÖ (fallback) | ‚úÖ (fallback) |
| Shell Commands | Full | Limited |

**CLI:** Run commands directly, read results
**IDE:** Provide commands for user to run, wait for confirmation, then read results

**Web Search (CLI only):**
```bash
python cursor-scripts/web_search.py "query"
```
- Uses Gemini with Google Search grounding
- Auto-logs to `cursor-web-search/CURSOR-WEB_YYYY-DD-MM.md`

---

### üî¨ Research Protocol (Multi-Source Grounding)

**When to trigger:** Any question requiring factual accuracy, technical details, best practices, or current information.

**Mandatory source order:**

| Step | Source | Method | When to Use |
|------|--------|--------|-------------|
| 1 | **Codebase** | Grep/Glob/Read | Always first - check existing code/patterns |
| 2 | **Local Docs** | Read `.md` files | Project-specific guides, READMEs |
| 3 | **Web Search** | `web_search.py` (CLI) | Queries, discovery, external info |
| 4 | **WebFetch** | `WebFetch` tool | Known URLs, specific doc pages |
| 5 | **Browser** | Browser MCP tools | JS-rendered pages, interaction needed |

**Web tool hierarchy:**
| Tool | Best For | Speed | When to Use |
|------|----------|-------|-------------|
| `web_search.py` | Queries, discovery | Fast | Primary - Gemini + Google grounding |
| `WebFetch` | Specific URLs | Medium | Secondary - direct fetch of known pages |
| Browser MCP | JS pages, interaction | Slow | Fallback - when above return incomplete |

**Use Browser MCP when:**
- Page requires JavaScript rendering (WebFetch returns incomplete/empty)
- Need to navigate or interact with the page
- Visual verification needed
- Other methods fail or return partial data

**Execution rules:**
- **Run sources in parallel** when independent (e.g., grep codebase + read docs simultaneously)
- **Cite sources** in response (file paths, URLs from web search)
- **Skip steps** only if clearly irrelevant (e.g., skip codebase for "what's the weather")
- **Escalate to browser** only when faster methods fail

**Example triggers:**
- "How do I use X?" ‚Üí Codebase (existing examples) ‚Üí Docs ‚Üí Web (official docs)
- "What's the best way to Y?" ‚Üí Codebase (current approach) ‚Üí Web (best practices)
- "Is Z still supported?" ‚Üí Web (latest info) ‚Üí Docs (local notes)

**CLI execution:**
```bash
# Codebase search
rg "pattern" --type py

# Web search (grounded)
python cursor-scripts/web_search.py "query"
```

---

### üîí Sensitive Files

**Never read unless explicitly debugging auth:**
- `.env` - API credentials

---

### üìÑ Citing Sources

| Can Cite | Cannot Cite |
|----------|-------------|
| `.py`, `.js`, `.ipynb` (code) | `.cursorrules` |
| `.md`, `.txt` (docs) | `.env`, `.gitignore` |
| `README.md` | `.cursor/` files |

---

## üìö Reference

### Tool Paths

```bash
# Usage tracking
python cursor-scripts/cursor_usage.py import   # Import CSV
python cursor-scripts/cursor_usage.py report   # Usage report
python cursor-scripts/cursor_usage.py quota    # Quota status
python cursor-scripts/cursor_usage.py budget   # Daily budget
python cursor-scripts/cursor_usage.py alerts   # Threshold alerts
python cursor-scripts/cursor_usage.py reminder # CSV reminder

# Chat export
./cursor-scripts/export-chat.sh

# New chat (export + clear)
./cursor-scripts/cursor-new-chat.sh

# Web search
python cursor-scripts/web_search.py "query"

# Update Cursor CLI
./cursor-scripts/update-cursor.sh
```

### Learning Tools

**Flashcard System (`review.py`):**
```bash
python cursor-scripts/review.py --today        # Show due cards
python cursor-scripts/review.py --add "Q" "A"  # Add card
python cursor-scripts/review.py --quiz         # Interactive review
python cursor-scripts/review.py --stats        # Review statistics
```

**Startup Cards (`startup_cards.py`):**
```bash
python cursor-scripts/startup_cards.py          # Full digest + quiz
python cursor-scripts/startup_cards.py --reveal # Show quiz answer
```

**Data storage:** `cursor-data/flashcards.json`

### Model Selection

**‚ö†Ô∏è BEFORE recommending ANY model:**
1. **Fetch latest benchmarks** ‚Äî don't rely on outdated knowledge
2. **Infer task type** from user's question
3. **Follow the workflow below**

**Workflow:**

1. **Automatically fetch latest benchmarks:**
   ```bash
   python cursor-scripts/get_model_benchmarks.py [task_type]
   ```
   - `task_type`: `coding`, `reasoning`, `writing`, `fast`, or `general`
   - Infer from user's question if not specified

2. **Make recommendation based on:**
   - Latest third-party benchmarks (LMSYS Arena, SWE-bench, HumanEval, etc.)
   - Task requirements (coding, reasoning, speed, cost)
   - Cost considerations (from latest pricing data)
   - User's specific needs

### Git Commits

**‚ö†Ô∏è BEFORE committing, follow `COMMIT-CONVENTIONS.md`:**
- Use tags: `add:`, `update:`, `fix:`, `refactor:`, `style:`, `revert:`
- Format: `<tag>: <description>`
- Date format in filenames: `YYYY-DD-MM`

### Documentation Files

- `COMMIT-CONVENTIONS.md` - Git commit message format
- `cursor-scripts/` - CLI tools and utilities
- `cursor-chats/` - Conversation logs
- `cursor-web-search/` - Web search logs
- `cursor-data/` - Flashcard and learning data

### Status

‚úÖ Usage Tracking: Ready
‚úÖ Chat Export: Ready
‚úÖ Web Search: Requires `GEMINI_API_KEY` in `.env`
‚úÖ Learning Tools: `review.py`, `startup_cards.py`
‚úÖ Model Benchmarks: `get_model_benchmarks.py`
‚úÖ Browser MCP: Available as fallback
